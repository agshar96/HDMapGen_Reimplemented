<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Reimplementation of HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps">
  <meta name="keywords" content="HDMapGen, Graph-generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HDMapGen: Reimplementation and Experiments</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <h1 class="title is-1 publication-title">HDMapGen: Reimplementation and Experiments</h1>
                <div class="is-size-4 publication-authors pb-3">
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/agniv-sharma-816063113/">Agniv Sharma</a></span>
                </div>
                <div class="is-size-5 publication-authors">
                    Special thanks to
                    <span class="author-block">
                        <a href="https://danieldauner.github.io/">Daniel Dauner, </a></span>
                    </span>
                    <span>
                        <a href="https://kashyap7x.github.io/">Kashyap Chitta,</a></span>
                    </span>
                    <span>
                        <a href="https://www.cvlibs.net/">Andreas Geiger</a>
                    </span>
                  
                </div>
      
                <div class="is-size-5 publication-authors py-2">
                  <span class="author-block">Tübingen AI Center</span>
                </div>
      
                <div class="column has-text-centered">
                  <div class="publication-links">
                    <!-- Paper Link. -->
                    <span class="link-block">
                        <a href="https://arxiv.org/abs/2106.14880"
                            class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                <i class="ai ai-arxiv"></i>
                                </span>
                                <span>Original Paper</span>
                        </a>
                    </span>
                    <!-- Code Link. -->
                    <span class="link-block">
                        <a href="https://github.com/agshar96/HDMapGen"
                            class="external-link button is-normal is-rounded is-dark">
                              <span class="icon">
                                <i class="fab fa-github"></i>
                              </span>
                              <span>Code</span>
                        </a>
                    </span>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>
    
    
    <section class="hero" id="Abstract">
        <div class="hero-body">
          <div class="container pb-4">
            <h2 class="title is-3" style="font-family: Calibri;">Abstract</h2>
            <p>
              In the following project, we present the first reimplementation of the paper, 
              <a href="https://arxiv.org/abs/2106.14880" class="black-link">HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps</a>,  a autoregressive graph neural network model for generating high-definition (HD) 2D road maps. The project presents a comprehensive analysis of modifications made to the original model to ensure functionality and effectiveness. Furthermore, we contribute to the field by releasing the source code for our reimplementation alongside pre-trained weights specifically optimized for the nuPlan dataset.
            </p>
          </div>
          <div class="container is-max-desktop has-text-centered">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item-1">
                <img src="./static/images/Trained_viz/1.png" alt="HDMapGen result 1"/>
              </div>
              <div class="item-2">
                <img src="./static/images/Trained_viz/2.png" alt="HDMapGen result 2"/>
              </div>
              <div class="item-3">
                <img src="./static/images/Trained_viz/3.png" alt="HDMapGen result 3"/>
              </div>
              <div class="item-4">
                <img src="./static/images/Trained_viz/4.png" alt="HDMapGen result 4"/>
              </div>
              <div class="item-5">
                <img src="./static/images/Trained_viz/5.png" alt="HDMapGen result 5"/>
              </div>
              <div class="item-6">
                <img src="./static/images/Trained_viz/6.png" alt="HDMapGen result 6"/>
              </div>
              <div class="item-7">
                <img src="./static/images/Trained_viz/7.png" alt="HDMapGen result 7"/>
              </div>
              <div class="item-8">
                <img src="./static/images/Trained_viz/8.png" alt="HDMapGen result 8"/>
              </div>
              <div class="item-9">
                <img src="./static/images/Trained_viz/9.png" alt="HDMapGen result 9"/>
              </div>
              <div class="item-10">
                <img src="./static/images/Trained_viz/10.png" alt="HDMapGen result 10"/>
              </div>
              <div class="item-11">
                <img src="./static/images/Trained_viz/11.png" alt="HDMapGen result 11"/>
              </div>
              <div class="item-12">
                <img src="./static/images/Trained_viz/12.png" alt="HDMapGen result 12"/>
              </div>
              <div class="item-13">
                <img src="./static/images/Trained_viz/13.png" alt="HDMapGen result 13"/>
              </div>
              <div class="item-14">
                <img src="./static/images/Trained_viz/14.png" alt="HDMapGen result 14"/>
              </div>
              <div class="item-15">
                <img src="./static/images/Trained_viz/15.png" alt="HDMapGen result 15"/>
              </div>
              <div class="item-16">
                <img src="./static/images/Trained_viz/16.png" alt="HDMapGen result 16"/>
              </div>
              <div class="item-17">
                <img src="./static/images/Trained_viz/17.png" alt="HDMapGen result 17"/>
              </div>
              <div class="item-18">
                <img src="./static/images/Trained_viz/18.png" alt="HDMapGen result 18"/>
              </div>
              <div class="item-19">
                <img src="./static/images/Trained_viz/19.png" alt="HDMapGen result 19"/>
              </div>
              <div class="item-20">
                <img src="./static/images/Trained_viz/20.png" alt="HDMapGen result 20"/>
              </div>
              <div class="item-21">
                <img src="./static/images/Trained_viz/21.png" alt="HDMapGen result 21"/>
              </div>
              <div class="item-22">
                <img src="./static/images/Trained_viz/22.png" alt="HDMapGen result 22"/>
              </div>
              <div class="item-23">
                <img src="./static/images/Trained_viz/23.png" alt="HDMapGen result 23"/>
              </div>
              <div class="item-24">
                <img src="./static/images/Trained_viz/24.png" alt="HDMapGen result 24"/>
              </div>
            </div>
            <p>Figure 1: nuPlan Generations</p>
          </div>
        </div>
    </section>

    <section class="hero" id="Introduction">
      <div class="hero-body pb-3">
        <div class="container">
          <h2 class="title is-3">1. Introduction</h2>
          <p>The HDMapGen paper modifies the GRAN architecture to generate High-definition maps. As our base approach, we use a modified version of GraphRNN - an older auto-regressive architecture - to serve as our baseline. We found that graphRNN usually fails to learn even simpler road maps.</p>
          <div class="container is-max-desktop has-text-centered my-4">
            <image src="./static/images/Introduction/graphRNN.png" width="50%" height="50%"/>
            <p>Figure 2: GraphRNN fails to converge</p>
          </div>
          <p>The base performance of GRAN architecture on these maps is also unsatisfactory.</p>
          <div class="container is-max-desktop has-text-centered my-4">
            <image src="./static/images/Introduction/GRAN_base.png" width="50%" height="50%"/>
            <p>Figure 3: Base GRAN with embeddings also fails</p>
          </div>
          <p>We identified shortcomings in the GRAN architecture and addressed them through several modifications. The subsequent sections detail the impact of these changes on the final learning performance. It's important to note that similar modifications were not explored with GraphRNN.</p>
          <p>For most experiments, we present results on a simpler grid dataset, which is included in our base code. This grid graph serves as a valuable benchmark to test the model's core functionality. Our experiments reveal that results on the grid dataset generalize well to the larger nuPlan dataset, where the final network training takes place.</p>
          <p>In addition to the experiment details, we are also releasing our implementation along with trained weights for a subset of the nuPlan dataset. Weights for the model trained on the complete nuPlan dataset will be made available upon the release of the <a href="https://github.com/autonomousvision/sledge">"SLEDGE"</a> code.</p>
        </div>
      </div>
    </section>

    <section class="hero" id="Experiment">
      <div class="hero-body">
        <div class="container pb-3" id="Common">
          <!-- Main Heading -->
          <h2 class="title is-3">2. Experiments</h2>
          <p>The base GRAN model learns only the adjacency matrix. We first extend it by adding an additional head to predict node positions using L2 loss. Subsequently, the predicted node positions, along with the learned latent adjacency representation, were used to predict subnode positions, again with L2 loss.</p>
          <div class="container has-text-centered my-3">
            <div class="columns pt-3" style="margin-bottom: 0px;">
              <div class="column has-text-centered">
                <image src="./static/images/Experiments/common/subnode_3x3/test_graphs_subnodes_1.png" width="100%" height="100%"/>
                <p>(a) Subnodes</p>
              </div>
              <div class="column has-text-centered">
                <image src="./static/images/Experiments/common/subnode_3x3/test_graphs_1.png" width="100%" height="100%"/>
                <p>(b) Only Nodes with coordinates</p>
              </div>
            </div>
            <p class="pb-3">Figure 4: The base GRAN works successfully for 3x3 grid</p>
          </div>
          <p>While just making the modifications work for a small 3x3 grid, they fail for larger graphs, even for 5x5 grids:</p>
          <div class="container has-text-centered my-3">
            <div class="columns pt-3" style="margin-bottom: 0px;">
              <div class="column has-text-centered">
                <image src="./static/images/Experiments/common/5x5/subnode.png" width="100%" height="100%"/>
                <p>(a) Subnodes</p>
              </div>
              <div class="column has-text-centered">
                <image src="./static/images/Experiments/common/5x5/embed.png" width="100%" height="100%"/>
                <p>(b) Only Nodes with coordinates</p>
              </div>
            </div>
            <p class="pb-3">Figure 5: The base GRAN breaks even for slightly more complex grid</p>
          </div>
          <p>These limitations necessitated modifications to both the training strategies and model architecture choices.</p>
        </div>
        <div class="container pt-4 pb-3" id="Noise">
          <h3 class="title is-4">2.1 Adding noise to the node(and subnode) positions </h3>
          <p>While training on graphs larger than 5x5, we observed a critical issue. The training dataset solely included graphs with specific node positions (e.g., (1,1) or (1,2) etc). During the autoregressive generation process, any deviation from these exact positions – for instance, if (1,1) changed to (1.0059, 0.99956) – resulted in a complete collapse of the graph generation.</p>
          <div class="container is-max-desktop has-text-centered my-4" id="5x5_failure_video">
            <video poster="" id="5x5_break" autoplay controls muted loop playsinline height="50%">
              <source src="./static/images/Experiments/noise/5x5_break.mp4"
                      type="video/mp4">
            </video>
            <p>Figure 6: Deviation from training positions (28 sec) causes collapse</p>
          </div>
          <p>To address this challenge, we introduced a small amount of Gaussian noise to each node position. This intervention effectively helped the model learn a more generalizable underlying structure for the graphs.</p>
          <div class="container is-max-desktop has-text-centered my-4" id="5x5_failure_video">
            <video poster="" id="5x5_break" autoplay controls muted loop playsinline height="50%">
              <source src="./static/images/Experiments/noise/5x5 success.mp4"
                      type="video/mp4">
            </video>
            <p>Figure 7: Addition of noise stablizes the training and helps the generation</p>
          </div>
          <p>This approach is analogous to data augmentation techniques commonly employed in other deep learning applications However, in our case level of noise depends on the chosen node position representation. In dense normalized graphs, the standard deviation must be low. Conversely, sparser graphs necessitate a higher level of noise to encourage exploration of a wider range of positions during training.</p>
        </div>
        <div class="container pt-4 pb-3" id="Subnode">
          <h3 class="title is-4">2.2 Implementation of subnode prediction network</h3>
          <p>In the HDMapGen paper, authors learn the subnode prediction network and node prediction + adjacency prediction network separately. In contrast, our implementation leverages a joint training strategy.</p>
          <p>We employ L2 loss for subnode prediction, aiming for precise subnode coordinate forecasts when edges exist. Conversely, for non-existent edges, we predict subnode coordinates as (0,0). To achieve this, we incorporate an additional MLP (Multi-Layer Perceptron) within the network. This MLP takes two inputs:
            <ol class="my-2" style="padding-left:45px">
              <li style="padding-bottom: 5px">The positions of the two nodes between which subnodes are to be predicted.</li>
              <li style="padding-bottom: 5px">The latent representation of adjacency for the nodes, indicating the presence or absence of edges.</li>
            </ol>
          </p>
          <p>The MLP's output is then modulated by a weighting factor. This factor is 1 when the adjacency network predicts an existing edge, effectively enabling the subnode prediction. Conversely, the factor becomes 0 when no edge is predicted, essentially disregarding the subnode prediction in such scenarios.</p>
          <p>Thus in our model, a wrong prediction of an edge gives a larger loss because of the subnode network. So,  subnode predictions contribute to stabilizing the learning process for the adjacency matrix. Consequently, we can train the entire model jointly in a single step, eliminating the need for separate training stages as employed in HDMapGen.</p>
          <div class="container has-text-centered my-3">
            <div class="columns pt-3" style="margin-bottom: 0px;">
              <div class="column has-text-centered">
                <image src="./static/images/Experiments/subnode/Subnode_stablizes.png" width="100%" height="100%"/>
                <p>(a) Trained with subnodes</p>
              </div>
              <div class="column has-text-centered">
                <image src="./static/images/Experiments/subnode/embed_unstable.png" width="100%" height="100%"/>
                <p>(b) Trained without subnodes</p>
              </div>
            </div>
            <p class="pb-3">Figure 8: When training with subnodes for similar amount of epochs, we recieve more consistent generations.</p>
          </div>
        </div>
        <div class="container pt-4 pb-3" id="GMMLoss">
          <h3 class="title is-4">2.3 Replacing L2 loss with 2D Gaussian Mixture Model Loss</h3>
          <p>The authors of the HDMapGen paper employ GMM loss instead of the more common L2 loss function. While the paper does not explicitly discuss the rationale behind this choice, our experiments using L2 loss provide valuable insights into the potential advantages of GMM loss.</p>
          <p>Consider two discretized grids, one of size 3x3 and another of size 4x4, with node generation ordered using Depth-First Search (DFS). For the 3x3 grid, the DFS traversal would result in the following node generation sequence:</p>
          <p class="my-2" style="padding-left:20px">[1,1] → [1,0] → [0,0] → [0,1] → [0,2] → [1,2] → and so on</p>
          <p>Similarly, for the 4x4 grid, the sequence would be:</p>
          <p class="my-2" style="padding-left:20px">[1,1] → [1,0] → [0,0] → [0,1] → [0,2] → [0,3] → and so on</p>
          <p>Crucially, for the same underlying sequence, the model can potentially predict two distinct positions due to the discretization of the grid space. When using L2 loss, the model might be inclined to favor an average position between these two valid options. However, ideally, we would prefer the model to definitively select one of these positions.</p>
          <p>Given the limitations of L2 loss in this scenario, GMM loss emerges as a more suitable alternative. GMM loss allows the model to capture the inherent multi-modality of the problem, effectively representing the two possible prediction points within the grid space. This enables the model to make a more precise selection, aligning better with the desired outcome.</p>
          <!-- <div class="container has-text-centered my-3">
            <div class="columns pt-3" style="margin-bottom: 0px;">
              <div class="column has-text-centered">
                <video poster="" autoplay controls muted loop playsinline height="50%">
                  <source src="./static/images/Experiments/GMMLoss/L2_fail.mp4"
                          type="video/mp4">
                </video>
                <p>(a) L2 loss fails on a dataset of 3x3 with 4x4 grids</p>
              </div>
              <div class="column has-text-centered">
                <image src="./static/images/Experiments/GMMLoss/GMM_success.png" width="50%" height="100%"/>
                <p>(b) GMMLoss enables successful multigraph generations.</p>
              </div>
            </div>
            <p class="pb-3">Figure 8: Comparision of different losses</p>
          </div>
          </div> -->           
          <div class="container is-max-desktop has-text-centered my-4" id="L2_fail">
            <div class="container pb-2">
              <video poster="" autoplay controls muted loop playsinline height="50%">
                <source src="./static/images/Experiments/GMMLoss/L2_fail.mp4"
                        type="video/mp4">
              </video>
              <p>(a) L2 loss fails on a dataset of 3x3 with 4x4 grids</p>
            </div>
            <div class="container pb-2">
              <image src="./static/images/Experiments/GMMLoss/GMM_success.png" width="50%" height="50%"/>
              <p>(b) GMMLoss enables successful multigraph generations.</p>
            </div>
            <p>Figure 9: Comparision of different losses</p>
          </div> 

        </div>
        <div class="container pt-4 pb-3" id="StopNode">
          <h3 class="title is-4">2.4 Effect of stop node</h3>
          <p>The baseline implementation of GRAN lacks a dedicated "stop node" mechanism. During training, it records a distribution over the number of nodes present in various graphs within the dataset. At test time, it generates a graph with the maximum number of nodes observed during training. Subsequently, it samples the number of nodes from the learned distribution and discards any nodes exceeding that sample.</p>
          <p>For instance, if the training data comprises equally distributed graphs of sizes 3x3, 4x4, and 5x5, the model will consistently generate a 5x5 graph. It then samples the number of nodes from the set {9, 16, 25} and discards any nodes exceeding the sampled value. While this strategy proved effective for GRAN's task of solely generating adjacency matrices, it can introduce artifacts in our scenario.</p>
          <div class="container has-text-centered my-3">
            <div class="columns pt-3" style="margin-bottom: 0px;">
              <div class="column has-text-centered">
                <image src="./static/images/Experiments/stopNode/Fail_1.png" width="70%" height="70%"/>
                <p>(a) 4x4 graph preterminated</p>
              </div>
              <div class="column has-text-centered">
                <image src="./static/images/Experiments/stopNode/fail_2.png" width="70%" height="70%"/>
                <p>(b) 3x3 grid with extra generated nodes</p>
              </div>
            </div>
            <p class="pb-3">Figure 10: Failure cases because of no stop node</p>
          </div>
          <p>To address this limitation and facilitate task-specific generation, we introduce a novel "stop node" concept. This node connects to all previously generated nodes and serves as a signal for the model to terminate the generation process. The inclusion of this stop node ensures that the model generates graphs with the desired number of nodes, eliminating the generation of unnecessary nodes and associated artifacts.</p>
          <div class="container is-max-desktop has-text-centered my-4">
            <image src="./static/images/Experiments/stopNode/STOP_NODE_SUCCESS.png" width="70%" height="70%"/>
            <p>Figure 10: Introducing 'STOP NODE' leads to successful multigraph generation</p>
          </div>
        </div>
        <div class="container pt-4 pb-3" id="numMixtures">
          <h3 class="title is-4">2.5 Effect of number of mixtures</h3>
          <p>Our experiments revealed a positive correlation between the number of Gaussian mixtures employed in the GMM loss function and the resulting model performance. We hypothesize that this performance improvement will eventually plateau at a specific number of mixtures. However, due to training resource limitations, we were unable to exhaustively test a wider range of models on the entire nuPlan dataset.</p>
          <p>In lieu of a comprehensive evaluation on the full dataset, we utilized loss curves as a proxy to gauge model performance. By analyzing the loss curves, we could observe the impact of increasing the number of Gaussian mixtures on the model's ability to learn and minimize the loss function.</p>
          <div class="container is-max-desktop has-text-centered my-4">
            <image src="./static/images/Experiments/numMixtures/loss_curves.png" width="100%" height="100%"/>
            <p>Figure 11: Training Curve Comparision: The yellow curve corresponds to 20 mixtures</p>
          </div>
        </div>
    </section>

    <section class="hero" id="BibTeX">
      <div class="hero-body pb-3">
        <div class="container">
          <h2 class="title is-3">3. BibTeX</h2>
          <pre><code style="font-size: 18px;">@misc{agshar96HDMapGenReimplementation,
            author = {},
            title = {{H}{D}{M}ap{G}en: {R}eimplementation and {E}xperiments --- agshar96.github.io},
            howpublished = {\url{https://agshar96.github.io/HDMapGen_Reimplemented/}},
            year = {2024},
          }</code></pre>
        </div>
      </div>
    </section>

    <footer class="footer">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This website is licensed under a <a rel="license"
                                                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
              <p>
                This webpage template is from <a
                  href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> We sincerely thank 
                  <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template. 
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
    <!-- <section class="hero is-light is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel"> -->
    
</body>